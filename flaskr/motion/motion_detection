#!/usr/bin/env python3
# -*-coding:utf-8-*-
# by Jack Ran

from picamera.array import PiRGBArray
from picamera import PiCamera
import argparse
import warnings
import datetime
import imutils
import json
import time
import cv2

# imutils: 这是一个可以对图片进行各种操作的工具，比如伸缩变换，
#           拉伸，旋转等基本功能，用来帮助我们更好的使用OpenCV
# argparse: 是一个完整的参数处理库,当一个脚本执行的时候,后面可以跟参数,
#           用来触发一些其他的选项,当然这是在下面设定好的...


##############################################################################
#                                                                            #
#                               配置文件初始化                                 #
#                                                                            #
##############################################################################

# 创建解析器
ap = argparse.ArgumentParser()

ap.add_argument(
    "-c",
    "--conf",
    required=True,
    # 添加参数选项 ,后面的"required"是默认为True,
    help="path to the JSON configuration file",
)

# parse_args()进行一个参数解析
args = vars(ap.parse_args())

# 这里执行的结果是：Namespace(conf='conf.json'),这里可以看到，ap.parse_args()
# 是脚本后面传入的第一个参数显示了出来，而“conf”，是前面设置的关键字，但是我想把它提取出来，
# 怎么做呢？就用到了下面的“args”
ap.parse_args()

# 这里执行的结果是：“{'conf': 'conf.json'}”，这里很明显的看到了“args()”
# 函数把传入的对象的属性和属性值传进来了，并且是以字典的形式，那么这样就好办了，
# 通过后面的调用“args['conf']” 来调用conf对于的values，也就是配置文件的名字。

# 由于我们的文件是由字典组成的，因此提取里面的内容可以通过调用“key”的形式，
# 但是如何把他们先读出来呢？并且是以字典的形式取出来，这里就用到了“json”的“load”，
# 也就是在不改变文件当中字符的类型，把他们提取出来，并且提取出来的还是字典，保证原来的属性不变，
# 这里把conf.json的内容提取出来，并且赋给“conf”,这样就可以直接调用conf当中的key来找到他对应的values。
conf = json.load(open(args["conf"]))


###############################################################################
#                                                                             #
#                               摄像头初始化                                    #
#                                                                             #
###############################################################################

# 初始化摄像头，这里就不过多阐述了，就是设置一些初始值，
# 这里就直接调用“conf”的key，直接拿到对应的values，
camera = PiCamera()

# 注意这里是元组，因此需要将当中的列表表示的视频宽和高，变换为元组的形式，
# 因为在conf.json当中是“[640,480]”-->"(640,480)"这样是不是就好理解了？
camera.resolution = tuple(conf["resolution"])

camera.framerate = conf["fps"]

# 这里有个很重要的就是：“PIRGBArray：用来保存 RGB 图像”。
# 而他的格式就是“class picamera.array.PiRGBArray(camera, size=None)”，
# 这两个参数是必须添加，
rawCapture = PiRGBArray(camera, size=tuple(conf["resolution"]))


##############################################################################
#                                                                            #
#                           摄像头预热和计时器初始化                            #
#                                                                            #
##############################################################################

# 初始化摄像头，其实也就是睡眠几秒钟，这个都由你定
print
"[INFO] warming up..."
time.sleep(conf["camera_warmup_time"])
avg = None

lastUploaded = datetime.datetime.now()
# 记录下此时的秒数，后面需要计时，防止过于频繁的拍照和上传.

motionCounter = 0
# 初始化计数器，后面会用到，

second = time.time()
tuple_time = time.localtime(second)
# 时间戳，这里是为了在拍照的时候，以时间命名，防止冲突
time_zone = time.strftime("%Y-%m-%d_%H-%M-%S", tuple_time)


#############################################################################
#                                                                           #
#               对获取到的每一帧图像做初步处理（灰度转换--低通滤波）               #
#                                                                           #
#############################################################################

# 从摄像头逐帧捕获数据，类似于遍历。
for f in camera.capture_continuous(rawCapture, format="bgr", use_video_port=True):
    # 采集间隔序列：“capture_continuous”，这个作用就是相机会不断地捕捉图像，
    # 其中的“format：Write the raw image data to a file in 24-bit BGR format
    # （写的原始图像数据到一个文件中的24位BGR格式 ）”，这是一种捕获图片后保存的格式，为什么要用这个格式呢？？？
    # 因为 opencv 中图像格式是 bgr ，所以必须要确定格式，需要让OpenCV去可以读取这个每一帧画面 ；
    # “use_video_port：- 使用摄像头的图像或者视频端口进行图像捕获，默认为 False，表示使用图像端口。
    # 图像端口捕获速度慢（打开预览可知，捕获时会出现停顿现象），但是图像质量高；如果想要快速的捕获图像，
    # 使用视频端口，那么设为 True”，一般情况下，为了快速捕获对象，都采用了True。

    frame = f.array
    # 把每一帧采集到的画面。进行数组化，从而可以进行后面的图像处理操作

    timestamp = datetime.datetime.now()
    # 盖上时间戳，要和前面的“lastUploaded”，一会儿需要做一个减法，来推算时间

    text = "Unoccupied"
    # 先赋值一个关键字给“text”，后面需要通过判断“text”的内容来进行逻辑判断

    frame = imutils.resize(frame, width=500)
    # 官方解释：“resize：图像大小OpenCV是通过调用函数来完成cv2.resize。然而，需要特别小心，以确保保持长宽比。这一调整功能imutils保持纵横比，提供关键字参数的宽度和高度，所以图像可以调整到预定的宽度/高度的同时保持纵横比（1）和（2）保证图像的尺寸。”这里就是一个对图片的变换，并且还保持比例不变

    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    # cv2.cvtColor是OpenCV的颜色空间转换的方法，通常都是：“BGR<->Gray 和 BGR<->HSV ”之间的转换，而它的用法是：“cv2.cvtColor(input_image , flag),flag是转换类型：cv2.COLOR_BGR2GRAY,cv2.COLOR_BGR2HSV ”，其实就是刚刚数组化的图像进行一个灰度转换。

    gray = cv2.GaussianBlur(gray, (21, 21), 0)
    # 高斯模糊：其实就是低通滤波，就是把高频成分滤掉，然后就会显得图片模糊化了，这个是数字图像处理讲过的，在数学上，就是：“对图像做高斯模糊等同于将图像与高斯函数卷积。”其中的“（21,21）是高斯矩阵，或者说是高斯卷积的模板，用来做卷积>，从而把图像模糊处理。这里的“（21,21）”指的是矩阵的长和宽都是21，当标准差取0时OpenCV会根据高斯矩阵的尺寸自己计算。”

    #############################################################################
    #                                                                           #
    #           判断如果是第一帧图像那么就把它作为背景，后面程序略过            #
    #                                                                           #
    #############################################################################

    # 如果平均帧是None，初始化它,
    if avg is None:
        print
        "[INFO] starting background model..."

        avg = gray.copy().astype("float")
        # gray:灰度变换，copy:进行拷贝，astype:字段的数据类型转换，这里的意思是：“获得一幅新的图像,将原来的RGB(R,G,B)中的R,G,B统一用Gray替换，形成新的颜色RGB(Gray,Gray,Gray)，用它替换原来的RGB(R,G,B)就是灰度图了。然后进行一个拷贝，最后进行数据转换，把它变为浮点型”

        rawCapture.truncate(0)
        # 用法介绍："truncate() 方法用于截断文件，如果指定了可选参数 size，则表示截断文件为 size 个字符。 如果没有指定 size，则从当前位置起截断；截断之后 size 后面的所有字符被删除。"，可以看到这里截断的应该是所有字符。这里应该是把第一次保存的RGB图像截断了，也就是数组化的图像被截断了，应该是为了把这个第一次获取到的帧图像，作为背景，用来和后面的产生的帧图像进行对比，从而发现异样，进而判断是否有物体移动。因此这也是为什么第一次的初始化后，就把后面的程序“continue”跳过了。
        continue

    #####################################################################################
    #                                                                                   #
    #    执行下面的语句意味着背景已经获取，开始进行当前帧和背景的差异，并进行边缘检测   #
    #                                                                                   #
    #####################################################################################

    # 在当前帧和先前帧之间累积加权平均值，然后计算当前帧与运行平均值之间的差异。这里就是假设上面的第一帧已经被捕获，现在下面是：前面捕获的帧和现在的帧，进行一个对比，加权平均以后，看看误差是多少，当差异越大，说明当前这一帧比上面变化了很多，说明有运动的物体在镜头当中。

    cv2.accumulateWeighted(gray, avg, 0.5)
    # 目标跟踪方法：cv2.accumulateWeighted(src, dst, alpha[, mask]) → None
    # 参数解释：
    #          src – Input image as 1- or 3-channel, 8-bit or 32-bit floating point.
    #          dst – Accumulator image with the same number of channels as input image, 32-bit or 64-bit floating-point.
    #          alpha – Weight of the input image.
    #
    #          src:输入图像为1或3通道，8位或32位浮点。
    #          dst:具有相同数目的通道作为输入图像的32位累加器图像，32位或64位浮点型，也就是被比较的背景帧！！！
    #          alpha:输入图像的权重。
    # 解释：也就是我前面阐述过的，“gray”是当前的帧，进行灰度和模糊化处理后的图像，而“avg”是前一帧的图像，两者做一个差别比较，并且后面跟着加权系数，因为始终都是下一次帧之间的比较都是，前后两帧的比较，也就是背景会时刻改变的，因此加权系数“0.5”就好了，计算差异，也就是说权重越大，对前一帧，也就是背景帧的遗忘速度越快，这样就更不容易检测出和前一帧的差异，这里需要你认真理解。
    # 并且注意看到，这里是没有输出值的，这是因为这里做的加权最后又累加到“avg”当中，这也是为什么下面要把“avg”的类型再转换回“unit8”的类型，下面我贴出收藏的博客，对这个的解释，可以看下：
    #
    # 我们可以建立一个动态的背景模型，这个动态模型实现以下两个功能：
    #    1.  如果初始建立的背景模型中包含有前景物体，动态模型应该能够快速将前景物体的影响降低或消除掉；
    #    2.  对于背景模型中静止的物体位置改变或者新加入视频画面中静止的物体，动态模型应该能够快速觉察到这种变化，      并把这种改变纳入到下一轮的背景模型构建中。
    #
    # 基于这两个基本的要求，构建动态背景模型的步骤如下：
    #        1.  以初始第一帧作为第一个背景模型
    #        2.  检测第二帧中运动物体，得到前景图像
    #        3.  把第二帧图像抠除检测到的前景物体后，以一定比例系数累加到上一轮构建的背景模型中
    #        4.  更新背景模型，在随后帧上，重复1,2,3
    # 其中：
    #    当alpha取值为0.9时，此时新加入背景模型的新元素占比较大，对新的背景模型的影响也大，从上图可以看到，除有少许拖影外，基本跟上一帧图像特征一致。
    #    当alpha取值为0.2时,此时，新加入背景模型的元素占比较小，意味着之前加入的元素比重相应较大，累计的背景模型有很重的“鬼影”，每一个虚影代表了最近新加入背景模型的一个元素。

    frameDelta = cv2.absdiff(gray, cv2.convertScaleAbs(avg))
    # convertScaleAbs用法格式：cv2.convertScaleAbs(src[, dst[, alpha[, beta]]])，其中可选参数alpha是伸缩系数，beta是加到结果上的一个值。结果返回uint8类型的图片。avg会有负值，还有会大于255的值。而原图像是uint8，即8位无符号数。因此需要这个方法来进行格式上的转换，否则图像有误。
    # absdiff用法格式：cv2.absdiff(dilate,erode)，这里举了一个例子，就是腐蚀膨胀，在这里dilate是膨胀，erode是腐蚀，通过一个图片的两次不同加工，然后再相减，就得到的是图片当中边缘的的轮廓，也就是我们学过的“边缘检测”！！！相同的道理，这里通过用现在这一帧减去前面作为背景的一帧，而得到了物体的边缘“frameDelta”，很好理解的吧，如果有很多不同的地方，那么就会有很明显的边缘被检测出来，进而确定了有物体在移动。

    #########################################################################################
    #                                                                                       #
    #    对边缘检测出来的图片进行 二值化--膨胀--检测物体的轮廓--得到放有轮廓的数据的列表    #
    #                                                                                       #
    #########################################################################################

    # 对变化图像进行阀值化, 膨胀阀值图像来填补孔洞, 在阀值图像上找到轮廓线
    thresh = cv2.threshold(frameDelta, conf["delta_thresh"], 255, cv2.THRESH_BINARY)[1]
    # threshold：固定阈值二值化；具体用法：ret,dst = cv2.threshold(src, thresh, maxval, type)
    # 参数解释：
    #            src： 输入图，只能输入单通道图像，通常来说为灰度图
    #            dst： 输出图
    #            thresh： 阈值
    #            maxval： 当像素值超过了阈值（或者小于阈值，根据type来决定），所赋予的值
    #            type：二值化操作的类型，包含以下5种类型： cv2.THRESH_BINARY； cv2.THRESH_BINARY_INV； cv2.THRESH_TRUNC； cv2.THRESH_TOZERO；cv2.THRESH_TOZERO_INV
    #      其中：
    #             cv2.THRESH_BINARY（黑白二值）
    #             cv2.THRESH_BINARY_INV（黑白二值反转）
    #             cv2.THRESH_TRUNC （得到的图像为多像素值）
    #             cv2.THRESH_TOZERO
    #             cv2.THRESH_TOZERO_INV
    # 解释：这里是把上面做了减法，并且也就是说检测到物体边缘的图片，作为输入图片，然后进行二值化的处理，这里规定凡是大于阈值“5”的像素RGB颜色，全部显示白色，其余的显示黑色，这就是二值化，后面的“255”就是maxval，凡是超过阈值“5”的像素，全部都是变为“255”的RGB颜色，也就是白色，当然这个也可以指定，最后这个type的选择，可以看我收藏的博客，选择了“cv2.THRESH_BINARY”，也就是最常见的二值化，而后面的“()[1]”括号外面的[1],则是说明要显示的是“cv2.threshold”的第二个参数，也就是“ret,dst”当中的“dst”，这个地方多看看。多记记

    thresh = cv2.dilate(thresh, None, iterations=2)
    # cv2.dilate膨胀的方法，cv2.dilate(src, kernel[, dst[, anchor[, iterations]]]) -->dst。
    #  参数解释：
    #            src – Source image.
    #            dst – Destination image of the same size and type as src .
    #            element – Structuring element used for dilation. If element=Mat() , a 3 x 3 rectangular structuring element is used.
    #            anchor – Position of the anchor within the element. The default value (-1, -1) means that the anchor is at the element center.
    #            iterations – Number of times dilation is applied.
    #
    #             src:要输入的原始图像
    #             dst:输出的相同大小和类型的目标图像
    #             element:用于扩张的结构化元素。如果元= mat()，3 x 3的矩形结构元素的应用。
    #             anchor:锚点在元素中的位置。默认值（- 1，- 1）表示锚点位于元素中心。
    #             iterations:应用膨胀次数。
    # 解释：这里需要注意的是，输入的是上面经过二值化以后的边缘检测图，现在做一个膨胀，来填充图像当中的孔，这里的扩张结构化元素“element”显示的是“None”，后面的“iterations”膨胀的次数，这里选择了膨胀两次。

    (cnts, _) = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    # cv2.findContours()函数来查找检测物体的轮廓,原型方法：cv2.findContours(image, mode, method[, contours[, hierarchy[, offset ]]]) 返回两个值：contours：hierarchy，一个是轮廓本身，还有一个是每条轮廓对应的属性。
    # 参数介绍：
    #            image：输入的原始图像
    #            mode：表示轮廓的检索模式，有四种：
    #                 cv2.RETR_EXTERNAL表示只检测外轮廓
    #                 cv2.RETR_LIST检测的轮廓不建立等级关系
    #                 cv2.RETR_CCOMP建立两个等级的轮廓，上面的一层为外边界，里面的一层为内孔的边界信息。如果内孔内还有一个连通物体，这个物体的边界也在顶层。
    #                 cv2.RETR_TREE建立一个等级树结构的轮廓。
    #            method：轮廓的近似办法，有如下三种：
    #                 cv2.CHAIN_APPROX_NONE存储所有的轮廓点，相邻的两个点的像素位置差不超过1，即max（abs（x1-x2），abs（y2-y1））==1
    #                 cv2.CHAIN_APPROX_SIMPLE压缩水平方向，垂直方向，对角线方向的元素，只保留该方向的终点坐标，例如一个矩形轮廓只需4个点来保存轮廓信息
    #                 cv2.CHAIN_APPROX_TC89_L1，CV_CHAIN_APPROX_TC89_KCOS使用teh-Chinl chain 近似算法
    # 返回值参数介绍：
    #            contour：cv2.findContours()函数首先返回一个list，list中每个元素都是图像中的一个轮廓
    #            hierarchy：当函数还可返回一个可选的hiararchy结果，这是一个ndarray，其中的元素个数和轮廓个数相同，每个轮廓contours[i]对应4个hierarchy元素hierarchy[i][0] ~hierarchy[i][3]，分别表示后一个轮廓、前一个轮廓、父轮廓、内嵌轮廓的索引编号，如果没有对应项，则该值为负数。
    # 解析：这里我们可以看到，这里了选择了“cv2.RETR_EXTERNAL”，只检测外表轮廓。轮廓的压缩方法“cv2.CHAIN_APPROX_SIMPLE”，可以理解为：只保留一些点的元素，其余的线条轮廓都被近似为一个点。认真理解“轮廓近似”。

    #################################################################################
    #                                                                               #
    #           遍历contour列表中获取的轮廓值，并进行一个阈值的逻辑判断             #
    #                                                                               #
    #################################################################################

    for c in cnts:
        # 遍历轮廓线。可以看到“cnts”就是被上面查找到轮廓。并且进行了轮廓近似化以后，输出的列表，当中每个列表元素都是，做了边缘检测后的图像当中的每个轮廓
        # 遍历轮廓：为了找到图像中通过阀值测试的区域，我们进行简单的轮廓检测。随后遍历这些轮廓，看他们是否大于 min_area 。如果该区域足够大，那么我们可以表明我们已经在当前帧中找到了发生运动的区域。

        if cv2.contourArea(c) < conf["min_area"]:
            # 如果轮廓太小，忽略它。因此也就是可以设置，检测的精度，当这个配置文件“conf.json”当中的“min_area”，很小的话，那么就是再细微的轮廓，它也会检测，并且计数，这样很容易受到环境的干扰，进而误报信息，所以这个值需要具体测量来确定
            continue

        (x, y, w, h) = cv2.boundingRect(c)
        # 计算轮廓线的外框, 在当前帧上画出外框，并且更新文本，这里更新文本，意味着，这个轮廓是符合要求的，可以在后面进行一个计数统计，并且更新文本是为了后面的逻辑判断。
        # cv2.boundingRect(img)，是用来画边框的。这个函数很简单，img是一个二值图，也就是它的参数
        # 返回四个值，分别是x，y，w，h；
        # x，y是矩阵左上点的坐标，w，h是矩阵的宽和高

        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
        # cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)函数用来画出矩行
        # 参数解释:
        #        第一个参数：img是原图
        #        第二个参数：（x，y）是矩阵的左上点坐标
        #        第三个参数：（x+w，y+h）是矩阵的右下点坐标
        #        第四个参数：（0,255,0）是画线对应的rgb颜色
        #        第五个参数：2是所画的线的宽度

        text = "Occupied"

    #################################################################################
    #                                                                               #
    #               在当前帧进行一个文本标记，可能在视频预览当中有效                #
    #                                                                               #
    #################################################################################

    # 在当前帧上标记文本和时间戳
    ts = timestamp.strftime("%A %d %B %Y %I:%M:%S%p")
    cv2.putText(frame, "Room Status: {}".format(text), (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
    cv2.putText(frame, ts, (10, frame.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0, 0, 255), 1)
    # cv.PutText(img, text, org, font, color) → None
    # 参数介绍：
    #        img：图片源
    #        text：要写入的文本内容
    #        org：图片中文本字符串的左下角。
    #        font：字体的位置，可以通过“cv.InitFont”来确定，或者想程序当中，用数组的形式简单的确定位置就好了。
    #                cv.InitFont(cv.CV_FONT_HERSHEY_SCRIPT_SIMPLEX, 1, 1, 0, 3, 8)，说明：创建一个矩形，来让我们在图片上写文字，参数依次定义了文字类型，高，宽，字体厚度等。。
    #        fontFace：字体类型,下面列举所有的字体类型：
    #             FONT_HERSHEY_SIMPLEX, FONT_HERSHEY_PLAIN, FONT_HERSHEY_DUPLEX, FONT_HERSHEY_COMPLEX, FONT_HERSHEY_TRIPLEX, FONT_HERSHEY_COMPLEX_SMALL, FONT_HERSHEY_SCRIPT_SIMPLEX, or FONT_HERSHEY_SCRIPT_COMPLEX
    #        fontScale：字体缩放因子乘以字体特定的基块大小，用数字代表倍数
    #        color：字体颜色，这里用RGB表示
    #        thickness：用于绘制文本的行的粗细,数字代表粗细
    # 解释：这里只有一个需要注意的，就是字体位置，这个地方，设置的时候，因为考虑到图片已经被数组化，所以，通过array的方法“shape”来具体确定到底把字体放在哪个位置，shape[0]，例如：建立一个4×2的矩阵c, c.shape[1] 为第一维的长度，c.shape[0] 为第二维的长度。

    #################################################################################
    #                                                                               #
    #                       逻辑判断“text值、间隔时间和连续帧                       #
    #                                                                               #
    #################################################################################

    if text == "Occupied":
        # 检测text的内容是否为“occipied”，也就是通过这个关键字来判断上面的轮廓遍历是否有大于设定阈值的，有的话，这个“text”一定是会改变的。。。。

        if (timestamp - lastUploaded).seconds >= conf["min_upload_seconds"]:
            # 判断时间间隔，这里设置的作用是，当物体频繁移动，并且被检测到，不能总是一直执行下面的操作，比如拍照，难道一秒钟有16帧，意味着要一秒拍16次吗？显然是不可以的。。。

            motionCounter += 1
            # 判断包含连续运动的帧数是否已经足够多，这里其实就是判断当检测到连续的多少帧，都是轮廓大于配置文件当中设定的阈值，那么就进行下一步，也就会是拍照，因为有些情况，一秒内只有连续的三四帧是有物体运动检测到，这样可能就不准确。环境因素在当中，并且这个阈值“min_motion_frames”是在配置文件当中可以调的，用来调节灵敏度。这里需要了解一下。

            if motionCounter >= conf["min_motion_frames"]:

                camera.capture("/root/graduation_design/warning/camera/picture/warining_%s.png" % time_zone)

                lastUploaded = timestamp
                # 更新时间，为当前，便于下次计时

                motionCounter = 0
                # 更新连续检测到运动的帧数为0，便于下次检测

    else:
        motionCounter = 0
        # 如果没有达到配置文件“conf.json”当中的阈值的连续帧，那么就将这个计数器清零，从新计算。

    rawCapture.truncate(0)
